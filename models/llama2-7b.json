{
  "hidden": 4096,
  "feedforward": 11008,
  "seq_size": 4096,
  "attn_heads": 32,
  "kv_heads": 32,
  "attn_size": 128,
  "num_blocks": 32,
  "ffn_type": "gated",
  "norm_type": "rmsnorm",
  "hidden_act": "silu",
  "vocab_size": 32000,
  "pos_encoding": "rope",
  "tie_emb": false,
  "attn_type": "flash_attn",
  "dropout": false
}
